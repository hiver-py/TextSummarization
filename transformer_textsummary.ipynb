{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0274b5e1",
   "metadata": {},
   "source": [
    "# Using T5-Small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ceae8",
   "metadata": {},
   "source": [
    "Read CNN/dailymail data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866e6a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (C:\\Users\\Patrick\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffe85b",
   "metadata": {},
   "source": [
    "Load tokenizer from pretrained t5-small model. Prepare data by applying the tokenizer aswell as adding 'summarize: ' prefix for the T5 model to know what task to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e69e9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 1.20MB/s]\n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 1.36MB/s]\n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:00<00:00, 2.24MB/s]\n",
      " 37%|███▋      | 107/288 [00:52<01:36,  1.88ba/s]"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
    "\n",
    "def prepfunc(data):\n",
    "    inputs = ['summarize: ' + doc for doc in data['article']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(data['highlights'], max_length=128, truncation=True)\n",
    "        \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_data = raw_datasets.map(prepfunc, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42def982",
   "metadata": {},
   "source": [
    "Load pretrained Seq2Seq transformer t5-small and define training arguments for finetuning on CNN/dailymail data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aac299a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at C:\\Users\\Jakob/.cache\\huggingface\\transformers\\fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at C:\\Users\\Jakob/.cache\\huggingface\\transformers\\fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "\n",
    "batch_size = 4\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"model/\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    max_steps=20000,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf3971",
   "metadata": {},
   "source": [
    "Data collator pads the input to max length aswell as the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b436eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b39962",
   "metadata": {},
   "source": [
    "Fine tune the model on CNN/daily mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9adfbae0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: highlights, article, id.\n",
      "***** Running training *****\n",
      "  Num examples = 287113\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 39:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.017400</td>\n",
       "      <td>1.833804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/checkpoint-500\n",
      "Configuration saved in model/checkpoint-500\\config.json\n",
      "Model weights saved in model/checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-1000\n",
      "Configuration saved in model/checkpoint-1000\\config.json\n",
      "Model weights saved in model/checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-1000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-1000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-1500\n",
      "Configuration saved in model/checkpoint-1500\\config.json\n",
      "Model weights saved in model/checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-1500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-1500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-2000\n",
      "Configuration saved in model/checkpoint-2000\\config.json\n",
      "Model weights saved in model/checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-2000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-2000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-2500\n",
      "Configuration saved in model/checkpoint-2500\\config.json\n",
      "Model weights saved in model/checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-2500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-2500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-3000\n",
      "Configuration saved in model/checkpoint-3000\\config.json\n",
      "Model weights saved in model/checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-3000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-3000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-3500\n",
      "Configuration saved in model/checkpoint-3500\\config.json\n",
      "Model weights saved in model/checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-3500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-3500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-4000\n",
      "Configuration saved in model/checkpoint-4000\\config.json\n",
      "Model weights saved in model/checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-4000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-4000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-4500\n",
      "Configuration saved in model/checkpoint-4500\\config.json\n",
      "Model weights saved in model/checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-4500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-4500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-5000\n",
      "Configuration saved in model/checkpoint-5000\\config.json\n",
      "Model weights saved in model/checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-5000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-5000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-5500\n",
      "Configuration saved in model/checkpoint-5500\\config.json\n",
      "Model weights saved in model/checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-5500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-5500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-6000\n",
      "Configuration saved in model/checkpoint-6000\\config.json\n",
      "Model weights saved in model/checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-6000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-6000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-6500\n",
      "Configuration saved in model/checkpoint-6500\\config.json\n",
      "Model weights saved in model/checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-6500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-6500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-7000\n",
      "Configuration saved in model/checkpoint-7000\\config.json\n",
      "Model weights saved in model/checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-7000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-7000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-7500\n",
      "Configuration saved in model/checkpoint-7500\\config.json\n",
      "Model weights saved in model/checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-7500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-7500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-8000\n",
      "Configuration saved in model/checkpoint-8000\\config.json\n",
      "Model weights saved in model/checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-8000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-8000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-8500\n",
      "Configuration saved in model/checkpoint-8500\\config.json\n",
      "Model weights saved in model/checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-8500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-8500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-9000\n",
      "Configuration saved in model/checkpoint-9000\\config.json\n",
      "Model weights saved in model/checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-9000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-9000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-9500\n",
      "Configuration saved in model/checkpoint-9500\\config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-9500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-9500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-10000\n",
      "Configuration saved in model/checkpoint-10000\\config.json\n",
      "Model weights saved in model/checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-10000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-10000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-10500\n",
      "Configuration saved in model/checkpoint-10500\\config.json\n",
      "Model weights saved in model/checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-10500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-10500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-11000\n",
      "Configuration saved in model/checkpoint-11000\\config.json\n",
      "Model weights saved in model/checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-11000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-11000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-11500\n",
      "Configuration saved in model/checkpoint-11500\\config.json\n",
      "Model weights saved in model/checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-11500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-11500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-12000\n",
      "Configuration saved in model/checkpoint-12000\\config.json\n",
      "Model weights saved in model/checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-12000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-12000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-12500\n",
      "Configuration saved in model/checkpoint-12500\\config.json\n",
      "Model weights saved in model/checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-12500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-12500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-13000\n",
      "Configuration saved in model/checkpoint-13000\\config.json\n",
      "Model weights saved in model/checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-13000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-13000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-13500\n",
      "Configuration saved in model/checkpoint-13500\\config.json\n",
      "Model weights saved in model/checkpoint-13500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-13500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-13500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-13500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-14000\n",
      "Configuration saved in model/checkpoint-14000\\config.json\n",
      "Model weights saved in model/checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-14000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-14000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-14500\n",
      "Configuration saved in model/checkpoint-14500\\config.json\n",
      "Model weights saved in model/checkpoint-14500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-14500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-14500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-14500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-15000\n",
      "Configuration saved in model/checkpoint-15000\\config.json\n",
      "Model weights saved in model/checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-15000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-15000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-15500\n",
      "Configuration saved in model/checkpoint-15500\\config.json\n",
      "Model weights saved in model/checkpoint-15500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-15500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-15500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-15500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-16000\n",
      "Configuration saved in model/checkpoint-16000\\config.json\n",
      "Model weights saved in model/checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-16000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-16000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-16500\n",
      "Configuration saved in model/checkpoint-16500\\config.json\n",
      "Model weights saved in model/checkpoint-16500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-16500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-16500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-16500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-17000\n",
      "Configuration saved in model/checkpoint-17000\\config.json\n",
      "Model weights saved in model/checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-17000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-17000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-17500\n",
      "Configuration saved in model/checkpoint-17500\\config.json\n",
      "Model weights saved in model/checkpoint-17500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-17500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-17500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-17500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-18000\n",
      "Configuration saved in model/checkpoint-18000\\config.json\n",
      "Model weights saved in model/checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-18000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-18000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-18500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model/checkpoint-18500\\config.json\n",
      "Model weights saved in model/checkpoint-18500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-18500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-18500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-18500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-19000\n",
      "Configuration saved in model/checkpoint-19000\\config.json\n",
      "Model weights saved in model/checkpoint-19000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-19000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-19000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-19000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-19500\n",
      "Configuration saved in model/checkpoint-19500\\config.json\n",
      "Model weights saved in model/checkpoint-19500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-19500\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-19500\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-19500\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/checkpoint-20000\n",
      "Configuration saved in model/checkpoint-20000\\config.json\n",
      "Model weights saved in model/checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in model/checkpoint-20000\\special_tokens_map.json\n",
      "Copy vocab file to model/checkpoint-20000\\spiece.model\n",
      "Deleting older checkpoint [model\\checkpoint-18500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: highlights, article, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20000, training_loss=2.0457638305664063, metrics={'train_runtime': 2395.9735, 'train_samples_per_second': 33.389, 'train_steps_per_second': 8.347, 'total_flos': 1.2939756840419328e+16, 'train_loss': 2.0457638305664063, 'epoch': 0.28})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef463fa2",
   "metadata": {},
   "source": [
    "Function to generate summarys from the model aswell as calculate rouge score on the test set (only using 100 articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155bdfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2605f202f044e5a26f9fecb4245c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2873 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=0.45269070213897944, recall=0.3046731322131221, fmeasure=0.35370055264741673), mid=Score(precision=0.45559409119911287, recall=0.3068487168266617, fmeasure=0.3559190872385176), high=Score(precision=0.4587084306967446, recall=0.3089078768492098, fmeasure=0.3581396412177233)), 'rouge2': AggregateScore(low=Score(precision=0.20186763660438073, recall=0.13382569768340982, fmeasure=0.15620751835113758), mid=Score(precision=0.20473826227740516, recall=0.13573082860482827, fmeasure=0.1582638269059013), high=Score(precision=0.2075054989573958, recall=0.13773673314366097, fmeasure=0.16049956933082268)), 'rougeL': AggregateScore(low=Score(precision=0.32569699831784266, recall=0.22034729678546564, fmeasure=0.2552846586441875), mid=Score(precision=0.32850264444106503, recall=0.2223305291026974, fmeasure=0.25728627401222104), high=Score(precision=0.331489789220222, recall=0.22424767220791147, fmeasure=0.2593910118660761)), 'rougeLsum': AggregateScore(low=Score(precision=0.3986946872921526, recall=0.2678983133649044, fmeasure=0.31123756501537836), mid=Score(precision=0.40171970538748586, recall=0.26979350583814765, fmeasure=0.3133284255577259), high=Score(precision=0.40507927252319526, recall=0.27188285062985407, fmeasure=0.3156748438110293))}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('model/checkpoint-20000')\n",
    "model.to('cuda')\n",
    "\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "xtest = raw_datasets['test']\n",
    "#xtest = xtest.select(range(100))\n",
    "\n",
    "min_generated_length = 25\n",
    "max_generated_length = 50\n",
    "\n",
    "def generate_summary(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids.to('cuda'), \n",
    "                             attention_mask=inputs.attention_mask.to(\"cuda\"), \n",
    "                             min_length=min_generated_length, \n",
    "                             max_length=max_generated_length, \n",
    "                             length_penalty=3.0)\n",
    "\n",
    "    batch[\"pred\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return batch\n",
    "\n",
    "res = xtest.map(generate_summary, batched=True, batch_size=4, remove_columns=['article'])\n",
    "reckful = rouge.compute(predictions=res['pred'], references=res['highlights'])\n",
    "print(reckful)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcade8c",
   "metadata": {},
   "source": [
    "Show 10 article summarys performed by our model as well as the corresponding highlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5455d084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlight:  \n",
      " \n",
      " James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88 .\n",
      "\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " \"The Dukes of Hazzard\" ran until 1985 and spawned TV movies, an animated series and video games. \"Jimmy Best was the most constantly creative person I have ever known,\" co-star says  \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " A lawyer for Dr. Anthony Moschetto says the charges against him are baseless .\n",
      "Moschetto, 54, was arrested for selling drugs and weapons, prosecutors say .\n",
      "Authorities allege Moschetto hired accomplices to burn down the practice of former associate . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " \"None of anything in this case has any evidentiary value,\" Dr. Anthony Moschetto says. He is accused of selling drugs, a cardiologist and a cardiologist. He is accused of \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " \"No challenge poses more of a public threat than climate change,\" the President says .\n",
      "He credits the Clean Air Act with making Americans \"a lot\" healthier . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Obama took part in a roundtable discussion this week on climate change. He says the issue is a public health issue that affects all of us. Obama says the Clean Air Act and its amendments have reduced early deaths  \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Presidential hopeful's video, featuring gay couple, gets mature rating in Russia .\n",
      "Russian TV channel feared airing it would break the country's anti-gay propaganda law .\n",
      "Clinton announced her support for same-sex marriage in 2013 . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Russian TV Rain channel aired Hillary Clinton's first campaign video with a rating stamp. The video features about five seconds of two men holding hands. Clinton's former boss says the law is offensive. \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Raul Reyes: In seeking Latino vote, Marco Rubio his own worst enemy on two key issues: immigration reform, Cuba relations .\n",
      "He says on health care, climate change and other issues, he breaks from Latinos' positions. Polls show they don't favor him . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Marco Rubio is all in. He's seeking the Republican presidential nomination. Rubio has embraced a typical conservative approach to immigration. He says President Obama's executive action on immigration should be ended. Ru \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Critically acclaimed series \"Orphan Black\" returns .\n",
      "\"Turn: Washington's Spies\" starts a second season .\n",
      "\"Game of Thrones\" is back for season five . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " \"Orphan Black\" kicks off the new season with six things to watch in the week ahead. The show is one of the most critically acclaimed shows on TV. The show's debut is on Saturday the 18th  \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " The ramp agent fell asleep in the plane's cargo hold .\n",
      "He can no longer work on Alaska Airlines flights . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " The crew and passengers reported unusual banging from the belly of the Boeing 737. The ramp agent was an employee of Menzies Aviation, a contractor for Alaska Airlines. The pilot said he would make an emergency landing \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Mullah Omar, the reclusive founder of the Afghan Taliban, is still in charge, a new biography claims .\n",
      "An ex-Taliban insider says there have been rumors that the one-eyed militant is dead . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " \"The Taliban has a huge leadership problem at a critical political moment,\" says a Taliban insider. The Taliban's \"Cultural Commission\" released the 11-page document in several different translations. The \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Michelle MacLaren is no longer set to direct the first \"Wonder Woman\" theatrical movie .\n",
      "MacLaren left the project over \"creative differences\"\n",
      "Movie is currently set for 2017 . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " The movie, starring Gal Gadot in the title role of the Amazon princess, is still set for release on June 23, 2017. The movie, starring Gal Gadot in the title role of the Amazon princess, \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " British tabloid releases video it says shows the robbery being carried out .\n",
      "British police say they didn't respond to a burglar alarm in jewelry district .\n",
      "Police give no value of the amount taken in the heist in London's jewelry district . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Police say they knew a burglar alarm went off but didn't respond. Police say they have not released any video of the heist. Police say they have not released any video of the heist. Police say \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Video of Toya Graham going to a protest and forcefully removing her son went viral, drew a lot of praise .\n",
      "The single mother of six tells CNN her son was scolded that he wasn't brought up that way .\n",
      "Michael Singleton says he knows his mom was trying to protect him . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " \"Not at all,\" Toya Graham tells CNN's \"Anderson Cooper 360 \" She says her son was embarrassing herself by wearing a mask and hoodie. The mother of six says she didn' \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Obsidian can produce cutting edges many times finer than even the best steel scalpels .\n",
      "Some surgeons still use the blades in procedures today . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " s a blind test on the cutting power of obsidian. The surgical technology used to carry out the primitive surgery was made from one of the sharpest substances found in nature. Dr. Lee Green says obsidian can \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Saudi general says more than 1,200 airstrikes since campaign began March 26 .\n",
      "Three Saudis were killed in attack on border position, source tells CNN . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Saudi officials say 385 civilians have been killed and 342 others wounded. Yemen has been descending into chaos since Houthi rebels forced Yemeni President Abdu Rabu Mansour Hadi from power in January  \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Most Americans say businesses should not discriminate against same-sex weddings .\n",
      "Public opinion has shifted on the issue since last fall .\n",
      "Indiana passed and later changed its religious freedom law after public outcry . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " to serve gay and lesbian couples, while 41% say they should be allowed to refuse service. 57% of Democrats and independents say wedding-related businesses should be required to serve same-sex couples. 57% say \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Opening statements are scheduled Monday in the trial of James Holmes .\n",
      "Jury selection took three months .\n",
      "Holmes faces 165 counts in the movie theater massacre that killed 12 people . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " ,,,,,, a new Batman movie, \"The Dark Knight Rises,\" in June 2012. Holmes admits to the shootings, but says he was insane at the time  \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Death toll in Nepal climbs above 4,600, officials say, with more than 9,000 injured .\n",
      "Shattered villages near epicenter are hard to reach, says aid worker in the area .\n",
      "More bad weather is forecast for the region in the coming days . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " , a trekking association official says. More than 4,600 people died in a landslide in Nepal, Nepal. Up to 200 people were feared missing as a result of a landslide, \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " In Baltimore, after the death of Freddie Gray, riots erupted, cars were set on fire and 200 arrests were made .\n",
      "Eric Liu: Liberals and conservatives react predictably, see the riots as confirmation of their views .\n",
      "It's time to push each other out of our ideological and identity comfort zones and change the status quo, he says . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " : We set Baltimore on fire this time, we brutalized black bodies. We threw stones at policemen, we threw stones at citizens. We created camera-ready chaos, and we replayed the images \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " The controversy over Indiana's religious freedom law is complicated .\n",
      "Some factors you might have not considered . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " the Indiana law, but it's not a copy of the federal law. The controversy is a wedge because the debate over religious freedom and gay rights is always heated. The key difference in the Indiana law is that it \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Six people taken hostage in a kosher market siege say media outlet endangered their lives .\n",
      "They hid in a cold room during the attack in Paris by gunman Amedy Coulibaly . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " , a French media outlet says. The lawsuit is filed March 27. The media outlet is accused of broadcasting live during the siege. The hostages were hiding in a cold room during the siege. \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Police officers escort the funeral procession to the service .\n",
      "Scott's family did not attend his visitation; they need privacy, mayor says .\n",
      "Police meet with the man who was a passenger in his car when it was pulled over . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Walter Scott's body was transported to his South Carolina funeral service. The funeral service was held in Charleston, South Carolina. The death of Scott was \"motivated by racial prejudice,\" a police officer says. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(res['highlights'][:20],res['pred'][:20]):\n",
    "    print('Highlight: ','\\n' ,'\\n', i, '\\n')\n",
    "    print('Summary: ', '\\n','\\n',j, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2c6a0",
   "metadata": {},
   "source": [
    "# Using BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e1b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Patrick\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234\\cache-feb9d3bdaebc6064.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Patrick\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234\\cache-463372f73f0ab15a.arrow\n",
      "100%|██████████| 12/12 [00:04<00:00,  2.61ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "def prepfunc(data):\n",
    "    inputs = [doc for doc in data['article']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(data['highlights'], max_length=128, truncation=True)\n",
    "        \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_data = raw_datasets.map(prepfunc, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a21f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
    "\n",
    "batch_size = 4\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"model-bArt/\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    max_steps=100000,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747fdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925e947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, article, highlights.\n",
      "***** Running training *****\n",
      "  Num examples = 287113\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100000\n",
      "  0%|          | 500/100000 [02:32<7:43:30,  3.58it/s]Saving model checkpoint to model-bArt/checkpoint-500\n",
      "Configuration saved in model-bArt/checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5469, 'learning_rate': 1.9900400000000003e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-3500] due to args.save_total_limit\n",
      "  1%|          | 1000/100000 [04:52<7:42:07,  3.57it/s]Saving model checkpoint to model-bArt/checkpoint-1000\n",
      "Configuration saved in model-bArt/checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3557, 'learning_rate': 1.98004e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-4000] due to args.save_total_limit\n",
      "  2%|▏         | 1500/100000 [07:12<7:34:25,  3.61it/s] Saving model checkpoint to model-bArt/checkpoint-1500\n",
      "Configuration saved in model-bArt/checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2938, 'learning_rate': 1.9700400000000002e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-1500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-4500] due to args.save_total_limit\n",
      "  2%|▏         | 2000/100000 [09:23<6:34:21,  4.14it/s] Saving model checkpoint to model-bArt/checkpoint-2000\n",
      "Configuration saved in model-bArt/checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.249, 'learning_rate': 1.9600400000000003e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-500] due to args.save_total_limit\n",
      "  2%|▎         | 2500/100000 [11:35<6:34:06,  4.12it/s] Saving model checkpoint to model-bArt/checkpoint-2500\n",
      "Configuration saved in model-bArt/checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2276, 'learning_rate': 1.95004e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-1000] due to args.save_total_limit\n",
      "  3%|▎         | 3000/100000 [13:37<4:00:05,  6.73it/s] Saving model checkpoint to model-bArt/checkpoint-3000\n",
      "Configuration saved in model-bArt/checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2025, 'learning_rate': 1.94004e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-1500] due to args.save_total_limit\n",
      "  4%|▎         | 3500/100000 [16:07<7:34:11,  3.54it/s] Saving model checkpoint to model-bArt/checkpoint-3500\n",
      "Configuration saved in model-bArt/checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2087, 'learning_rate': 1.9300400000000002e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-2000] due to args.save_total_limit\n",
      "  4%|▍         | 4000/100000 [18:30<7:04:51,  3.77it/s] Saving model checkpoint to model-bArt/checkpoint-4000\n",
      "Configuration saved in model-bArt/checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1939, 'learning_rate': 1.9200600000000004e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-2500] due to args.save_total_limit\n",
      "  4%|▍         | 4500/100000 [20:40<6:18:09,  4.21it/s] Saving model checkpoint to model-bArt/checkpoint-4500\n",
      "Configuration saved in model-bArt/checkpoint-4500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1713, 'learning_rate': 1.91006e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-4500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-3000] due to args.save_total_limit\n",
      "  5%|▌         | 5000/100000 [22:47<6:56:53,  3.80it/s] Saving model checkpoint to model-bArt/checkpoint-5000\n",
      "Configuration saved in model-bArt/checkpoint-5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1757, 'learning_rate': 1.9000600000000002e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-5000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-3500] due to args.save_total_limit\n",
      "  6%|▌         | 5500/100000 [24:55<6:00:32,  4.37it/s] Saving model checkpoint to model-bArt/checkpoint-5500\n",
      "Configuration saved in model-bArt/checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1713, 'learning_rate': 1.8900600000000003e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-5500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-4000] due to args.save_total_limit\n",
      "  6%|▌         | 6000/100000 [26:56<6:13:23,  4.20it/s] Saving model checkpoint to model-bArt/checkpoint-6000\n",
      "Configuration saved in model-bArt/checkpoint-6000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1485, 'learning_rate': 1.88006e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-6000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-4500] due to args.save_total_limit\n",
      "  6%|▋         | 6500/100000 [29:00<6:11:05,  4.20it/s] Saving model checkpoint to model-bArt/checkpoint-6500\n",
      "Configuration saved in model-bArt/checkpoint-6500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1555, 'learning_rate': 1.8700600000000002e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-6500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-5000] due to args.save_total_limit\n",
      "  7%|▋         | 7000/100000 [30:59<5:52:16,  4.40it/s] Saving model checkpoint to model-bArt/checkpoint-7000\n",
      "Configuration saved in model-bArt/checkpoint-7000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1321, 'learning_rate': 1.86008e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-7000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-5500] due to args.save_total_limit\n",
      "  8%|▊         | 7500/100000 [33:19<7:26:04,  3.46it/s] Saving model checkpoint to model-bArt/checkpoint-7500\n",
      "Configuration saved in model-bArt/checkpoint-7500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1632, 'learning_rate': 1.85008e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-7500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-6000] due to args.save_total_limit\n",
      "  8%|▊         | 8000/100000 [35:42<7:31:07,  3.40it/s] Saving model checkpoint to model-bArt/checkpoint-8000\n",
      "Configuration saved in model-bArt/checkpoint-8000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1285, 'learning_rate': 1.8400800000000002e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-8000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-6500] due to args.save_total_limit\n",
      "  8%|▊         | 8500/100000 [37:45<5:52:37,  4.32it/s] Saving model checkpoint to model-bArt/checkpoint-8500\n",
      "Configuration saved in model-bArt/checkpoint-8500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1243, 'learning_rate': 1.83008e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-8500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-7000] due to args.save_total_limit\n",
      "  9%|▉         | 9000/100000 [39:51<5:44:44,  4.40it/s] Saving model checkpoint to model-bArt/checkpoint-9000\n",
      "Configuration saved in model-bArt/checkpoint-9000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1378, 'learning_rate': 1.82008e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-9000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-7500] due to args.save_total_limit\n",
      " 10%|▉         | 9500/100000 [41:55<5:54:41,  4.25it/s] Saving model checkpoint to model-bArt/checkpoint-9500\n",
      "Configuration saved in model-bArt/checkpoint-9500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1315, 'learning_rate': 1.8101e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-9500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-8000] due to args.save_total_limit\n",
      " 10%|█         | 10000/100000 [44:14<6:55:12,  3.61it/s]Saving model checkpoint to model-bArt/checkpoint-10000\n",
      "Configuration saved in model-bArt/checkpoint-10000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1291, 'learning_rate': 1.8001000000000003e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-8500] due to args.save_total_limit\n",
      " 10%|█         | 10500/100000 [46:39<6:40:26,  3.73it/s] Saving model checkpoint to model-bArt/checkpoint-10500\n",
      "Configuration saved in model-bArt/checkpoint-10500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1222, 'learning_rate': 1.7901e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-10500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-9000] due to args.save_total_limit\n",
      " 11%|█         | 11000/100000 [49:02<6:53:54,  3.58it/s] Saving model checkpoint to model-bArt/checkpoint-11000\n",
      "Configuration saved in model-bArt/checkpoint-11000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1258, 'learning_rate': 1.78012e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-11000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-9500] due to args.save_total_limit\n",
      " 12%|█▏        | 11500/100000 [51:20<6:27:23,  3.81it/s] Saving model checkpoint to model-bArt/checkpoint-11500\n",
      "Configuration saved in model-bArt/checkpoint-11500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1341, 'learning_rate': 1.77012e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-11500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-10000] due to args.save_total_limit\n",
      " 12%|█▏        | 12000/100000 [53:29<6:08:07,  3.98it/s] Saving model checkpoint to model-bArt/checkpoint-12000\n",
      "Configuration saved in model-bArt/checkpoint-12000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.103, 'learning_rate': 1.76012e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-12000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-10500] due to args.save_total_limit\n",
      " 12%|█▎        | 12500/100000 [55:40<6:08:37,  3.96it/s] Saving model checkpoint to model-bArt/checkpoint-12500\n",
      "Configuration saved in model-bArt/checkpoint-12500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0993, 'learning_rate': 1.75012e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-12500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-11000] due to args.save_total_limit\n",
      " 13%|█▎        | 13000/100000 [57:46<5:38:25,  4.28it/s] Saving model checkpoint to model-bArt/checkpoint-13000\n",
      "Configuration saved in model-bArt/checkpoint-13000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0984, 'learning_rate': 1.74012e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-13000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-11500] due to args.save_total_limit\n",
      " 14%|█▎        | 13500/100000 [59:57<6:05:03,  3.95it/s] Saving model checkpoint to model-bArt/checkpoint-13500\n",
      "Configuration saved in model-bArt/checkpoint-13500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0913, 'learning_rate': 1.7301200000000003e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-13500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-13500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-13500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-12000] due to args.save_total_limit\n",
      " 14%|█▍        | 14000/100000 [1:02:11<6:06:56,  3.91it/s] Saving model checkpoint to model-bArt/checkpoint-14000\n",
      "Configuration saved in model-bArt/checkpoint-14000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0826, 'learning_rate': 1.72014e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-14000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-12500] due to args.save_total_limit\n",
      " 14%|█▍        | 14500/100000 [1:04:25<6:09:29,  3.86it/s] Saving model checkpoint to model-bArt/checkpoint-14500\n",
      "Configuration saved in model-bArt/checkpoint-14500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0746, 'learning_rate': 1.7101400000000002e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-14500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-14500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-14500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-13000] due to args.save_total_limit\n",
      " 15%|█▌        | 15000/100000 [1:06:38<6:00:39,  3.93it/s] Saving model checkpoint to model-bArt/checkpoint-15000\n",
      "Configuration saved in model-bArt/checkpoint-15000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.091, 'learning_rate': 1.70014e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-15000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-13500] due to args.save_total_limit\n",
      " 16%|█▌        | 15500/100000 [1:08:47<5:48:19,  4.04it/s] Saving model checkpoint to model-bArt/checkpoint-15500\n",
      "Configuration saved in model-bArt/checkpoint-15500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0669, 'learning_rate': 1.69014e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-15500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-15500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-15500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-14000] due to args.save_total_limit\n",
      " 16%|█▌        | 16000/100000 [1:10:55<5:52:19,  3.97it/s] Saving model checkpoint to model-bArt/checkpoint-16000\n",
      "Configuration saved in model-bArt/checkpoint-16000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0873, 'learning_rate': 1.68014e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-16000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-14500] due to args.save_total_limit\n",
      " 16%|█▋        | 16500/100000 [1:13:10<5:54:09,  3.93it/s] Saving model checkpoint to model-bArt/checkpoint-16500\n",
      "Configuration saved in model-bArt/checkpoint-16500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0987, 'learning_rate': 1.6701400000000002e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-16500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-16500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-16500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-15000] due to args.save_total_limit\n",
      " 17%|█▋        | 17000/100000 [1:15:20<6:29:30,  3.55it/s] Saving model checkpoint to model-bArt/checkpoint-17000\n",
      "Configuration saved in model-bArt/checkpoint-17000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0599, 'learning_rate': 1.66014e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-17000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-15500] due to args.save_total_limit\n",
      " 18%|█▊        | 17500/100000 [1:17:44<6:34:34,  3.48it/s] Saving model checkpoint to model-bArt/checkpoint-17500\n",
      "Configuration saved in model-bArt/checkpoint-17500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0289, 'learning_rate': 1.65016e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-17500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-17500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-17500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-16000] due to args.save_total_limit\n",
      " 18%|█▊        | 18000/100000 [1:20:10<6:29:28,  3.51it/s] Saving model checkpoint to model-bArt/checkpoint-18000\n",
      "Configuration saved in model-bArt/checkpoint-18000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0801, 'learning_rate': 1.6401600000000002e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-18000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-16500] due to args.save_total_limit\n",
      " 18%|█▊        | 18500/100000 [1:22:31<5:14:16,  4.32it/s] Saving model checkpoint to model-bArt/checkpoint-18500\n",
      "Configuration saved in model-bArt/checkpoint-18500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0715, 'learning_rate': 1.6301600000000003e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-18500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-18500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-18500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-17000] due to args.save_total_limit\n",
      " 19%|█▉        | 19000/100000 [1:24:57<5:56:54,  3.78it/s] Saving model checkpoint to model-bArt/checkpoint-19000\n",
      "Configuration saved in model-bArt/checkpoint-19000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0755, 'learning_rate': 1.62016e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-19000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-19000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-19000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-17500] due to args.save_total_limit\n",
      " 20%|█▉        | 19500/100000 [1:27:11<5:57:46,  3.75it/s] Saving model checkpoint to model-bArt/checkpoint-19500\n",
      "Configuration saved in model-bArt/checkpoint-19500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.069, 'learning_rate': 1.6101800000000002e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-19500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-19500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-19500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-18000] due to args.save_total_limit\n",
      " 20%|██        | 20000/100000 [1:29:28<6:11:51,  3.59it/s] Saving model checkpoint to model-bArt/checkpoint-20000\n",
      "Configuration saved in model-bArt/checkpoint-20000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0587, 'learning_rate': 1.60018e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-20000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-18500] due to args.save_total_limit\n",
      " 20%|██        | 20500/100000 [1:31:53<6:13:36,  3.55it/s] Saving model checkpoint to model-bArt/checkpoint-20500\n",
      "Configuration saved in model-bArt/checkpoint-20500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0479, 'learning_rate': 1.59018e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-20500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-20500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-20500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-19000] due to args.save_total_limit\n",
      " 21%|██        | 21000/100000 [1:34:17<6:13:24,  3.53it/s] Saving model checkpoint to model-bArt/checkpoint-21000\n",
      "Configuration saved in model-bArt/checkpoint-21000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0561, 'learning_rate': 1.5801800000000002e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-21000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-21000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-21000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-19500] due to args.save_total_limit\n",
      " 22%|██▏       | 21500/100000 [1:36:41<6:06:12,  3.57it/s] Saving model checkpoint to model-bArt/checkpoint-21500\n",
      "Configuration saved in model-bArt/checkpoint-21500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0452, 'learning_rate': 1.57018e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-21500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-21500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-21500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-20000] due to args.save_total_limit\n",
      " 22%|██▏       | 22000/100000 [1:39:05<6:04:53,  3.56it/s] Saving model checkpoint to model-bArt/checkpoint-22000\n",
      "Configuration saved in model-bArt/checkpoint-22000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0448, 'learning_rate': 1.56018e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-22000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-22000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-22000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-20500] due to args.save_total_limit\n",
      " 22%|██▎       | 22500/100000 [1:41:23<4:55:38,  4.37it/s] Saving model checkpoint to model-bArt/checkpoint-22500\n",
      "Configuration saved in model-bArt/checkpoint-22500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0389, 'learning_rate': 1.55018e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-22500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-22500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-22500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-21000] due to args.save_total_limit\n",
      " 23%|██▎       | 23000/100000 [1:43:22<5:05:44,  4.20it/s] Saving model checkpoint to model-bArt/checkpoint-23000\n",
      "Configuration saved in model-bArt/checkpoint-23000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0707, 'learning_rate': 1.5401800000000002e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-23000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-23000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-23000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-21500] due to args.save_total_limit\n",
      " 24%|██▎       | 23500/100000 [1:45:38<5:06:28,  4.16it/s] Saving model checkpoint to model-bArt/checkpoint-23500\n",
      "Configuration saved in model-bArt/checkpoint-23500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0591, 'learning_rate': 1.5301800000000003e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-23500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-23500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-23500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-22000] due to args.save_total_limit\n",
      " 24%|██▍       | 24000/100000 [1:47:54<6:10:04,  3.42it/s] Saving model checkpoint to model-bArt/checkpoint-24000\n",
      "Configuration saved in model-bArt/checkpoint-24000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.047, 'learning_rate': 1.5201800000000002e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-24000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-22500] due to args.save_total_limit\n",
      " 24%|██▍       | 24500/100000 [1:50:13<4:49:27,  4.35it/s] Saving model checkpoint to model-bArt/checkpoint-24500\n",
      "Configuration saved in model-bArt/checkpoint-24500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0156, 'learning_rate': 1.5101800000000002e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-24500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-24500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-24500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-23000] due to args.save_total_limit\n",
      " 25%|██▌       | 25000/100000 [1:52:23<4:47:23,  4.35it/s] Saving model checkpoint to model-bArt/checkpoint-25000\n",
      "Configuration saved in model-bArt/checkpoint-25000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0379, 'learning_rate': 1.5002000000000001e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-25000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-23500] due to args.save_total_limit\n",
      " 26%|██▌       | 25500/100000 [1:54:34<5:54:02,  3.51it/s] Saving model checkpoint to model-bArt/checkpoint-25500\n",
      "Configuration saved in model-bArt/checkpoint-25500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0364, 'learning_rate': 1.4902e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-25500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-25500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-25500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-24000] due to args.save_total_limit\n",
      " 26%|██▌       | 26000/100000 [1:57:04<5:54:33,  3.48it/s] Saving model checkpoint to model-bArt/checkpoint-26000\n",
      "Configuration saved in model-bArt/checkpoint-26000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0228, 'learning_rate': 1.4802000000000002e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-26000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-26000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-26000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-24500] due to args.save_total_limit\n",
      " 26%|██▋       | 26500/100000 [1:59:33<5:57:22,  3.43it/s] Saving model checkpoint to model-bArt/checkpoint-26500\n",
      "Configuration saved in model-bArt/checkpoint-26500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0285, 'learning_rate': 1.4702000000000001e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-26500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-26500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-26500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-25000] due to args.save_total_limit\n",
      " 27%|██▋       | 27000/100000 [2:01:58<4:59:45,  4.06it/s] Saving model checkpoint to model-bArt/checkpoint-27000\n",
      "Configuration saved in model-bArt/checkpoint-27000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0182, 'learning_rate': 1.4602200000000002e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-27000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-27000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-27000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-25500] due to args.save_total_limit\n",
      " 28%|██▊       | 27500/100000 [2:04:07<5:01:49,  4.00it/s] Saving model checkpoint to model-bArt/checkpoint-27500\n",
      "Configuration saved in model-bArt/checkpoint-27500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0035, 'learning_rate': 1.45024e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-27500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-27500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-27500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-26000] due to args.save_total_limit\n",
      " 28%|██▊       | 28000/100000 [2:06:09<4:51:31,  4.12it/s] Saving model checkpoint to model-bArt/checkpoint-28000\n",
      "Configuration saved in model-bArt/checkpoint-28000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0275, 'learning_rate': 1.4402400000000001e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-28000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-28000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-28000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-26500] due to args.save_total_limit\n",
      " 28%|██▊       | 28500/100000 [2:08:14<4:39:38,  4.26it/s] Saving model checkpoint to model-bArt/checkpoint-28500\n",
      "Configuration saved in model-bArt/checkpoint-28500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0095, 'learning_rate': 1.43024e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-28500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-28500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-28500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-27000] due to args.save_total_limit\n",
      " 29%|██▉       | 29000/100000 [2:10:21<4:42:40,  4.19it/s] Saving model checkpoint to model-bArt/checkpoint-29000\n",
      "Configuration saved in model-bArt/checkpoint-29000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0317, 'learning_rate': 1.4202400000000001e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-29000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-29000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-29000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-27500] due to args.save_total_limit\n",
      " 30%|██▉       | 29500/100000 [2:12:48<5:41:19,  3.44it/s] Saving model checkpoint to model-bArt/checkpoint-29500\n",
      "Configuration saved in model-bArt/checkpoint-29500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0239, 'learning_rate': 1.41024e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-29500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-29500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-29500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-28000] due to args.save_total_limit\n",
      " 30%|███       | 30000/100000 [2:15:03<4:51:41,  4.00it/s] Saving model checkpoint to model-bArt/checkpoint-30000\n",
      "Configuration saved in model-bArt/checkpoint-30000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0342, 'learning_rate': 1.40024e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-30000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-28500] due to args.save_total_limit\n",
      " 30%|███       | 30500/100000 [2:17:25<5:55:00,  3.26it/s] Saving model checkpoint to model-bArt/checkpoint-30500\n",
      "Configuration saved in model-bArt/checkpoint-30500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0221, 'learning_rate': 1.3902400000000001e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-30500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-30500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-30500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-29000] due to args.save_total_limit\n",
      " 31%|███       | 31000/100000 [2:19:55<5:41:19,  3.37it/s] Saving model checkpoint to model-bArt/checkpoint-31000\n",
      "Configuration saved in model-bArt/checkpoint-31000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0168, 'learning_rate': 1.38024e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-31000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-31000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-31000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-29500] due to args.save_total_limit\n",
      " 32%|███▏      | 31500/100000 [2:22:16<4:52:23,  3.90it/s] Saving model checkpoint to model-bArt/checkpoint-31500\n",
      "Configuration saved in model-bArt/checkpoint-31500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0163, 'learning_rate': 1.37024e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-31500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-31500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-31500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-30000] due to args.save_total_limit\n",
      " 32%|███▏      | 32000/100000 [2:24:31<4:45:20,  3.97it/s] Saving model checkpoint to model-bArt/checkpoint-32000\n",
      "Configuration saved in model-bArt/checkpoint-32000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0166, 'learning_rate': 1.36024e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-32000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-32000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-32000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-30500] due to args.save_total_limit\n",
      " 32%|███▎      | 32500/100000 [2:26:42<4:45:33,  3.94it/s] Saving model checkpoint to model-bArt/checkpoint-32500\n",
      "Configuration saved in model-bArt/checkpoint-32500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0015, 'learning_rate': 1.3502400000000001e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-32500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-32500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-32500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-31000] due to args.save_total_limit\n",
      " 33%|███▎      | 33000/100000 [2:28:54<4:59:46,  3.73it/s] Saving model checkpoint to model-bArt/checkpoint-33000\n",
      "Configuration saved in model-bArt/checkpoint-33000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0325, 'learning_rate': 1.3402600000000001e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-33000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-33000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-33000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-31500] due to args.save_total_limit\n",
      " 34%|███▎      | 33500/100000 [2:31:00<4:37:38,  3.99it/s] Saving model checkpoint to model-bArt/checkpoint-33500\n",
      "Configuration saved in model-bArt/checkpoint-33500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.01, 'learning_rate': 1.33026e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-33500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-33500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-33500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-32000] due to args.save_total_limit\n",
      " 34%|███▍      | 34000/100000 [2:33:20<5:04:48,  3.61it/s] Saving model checkpoint to model-bArt/checkpoint-34000\n",
      "Configuration saved in model-bArt/checkpoint-34000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0042, 'learning_rate': 1.3202600000000001e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-34000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-34000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-34000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-32500] due to args.save_total_limit\n",
      " 34%|███▍      | 34500/100000 [2:35:44<5:05:40,  3.57it/s] Saving model checkpoint to model-bArt/checkpoint-34500\n",
      "Configuration saved in model-bArt/checkpoint-34500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9938, 'learning_rate': 1.31026e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-34500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-34500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-34500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-33000] due to args.save_total_limit\n",
      " 35%|███▌      | 35000/100000 [2:38:07<4:59:33,  3.62it/s] Saving model checkpoint to model-bArt/checkpoint-35000\n",
      "Configuration saved in model-bArt/checkpoint-35000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0251, 'learning_rate': 1.3002600000000002e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-35000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-35000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-35000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-33500] due to args.save_total_limit\n",
      " 36%|███▌      | 35500/100000 [2:40:30<4:57:48,  3.61it/s] Saving model checkpoint to model-bArt/checkpoint-35500\n",
      "Configuration saved in model-bArt/checkpoint-35500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0013, 'learning_rate': 1.29026e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-35500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-35500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-35500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-34000] due to args.save_total_limit\n",
      " 36%|███▌      | 36000/100000 [2:42:36<4:52:52,  3.64it/s] Saving model checkpoint to model-bArt/checkpoint-36000\n",
      "Configuration saved in model-bArt/checkpoint-36000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0021, 'learning_rate': 1.28026e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-36000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-36000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-36000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-34500] due to args.save_total_limit\n",
      " 36%|███▋      | 36500/100000 [2:44:58<4:53:06,  3.61it/s] Saving model checkpoint to model-bArt/checkpoint-36500\n",
      "Configuration saved in model-bArt/checkpoint-36500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9816, 'learning_rate': 1.2702600000000001e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-36500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-36500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-36500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-35000] due to args.save_total_limit\n",
      " 37%|███▋      | 37000/100000 [2:47:21<4:51:31,  3.60it/s] Saving model checkpoint to model-bArt/checkpoint-37000\n",
      "Configuration saved in model-bArt/checkpoint-37000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0117, 'learning_rate': 1.26026e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-37000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-37000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-37000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-35500] due to args.save_total_limit\n",
      " 38%|███▊      | 37500/100000 [2:49:43<4:54:17,  3.54it/s] Saving model checkpoint to model-bArt/checkpoint-37500\n",
      "Configuration saved in model-bArt/checkpoint-37500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0157, 'learning_rate': 1.2502800000000002e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-37500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-37500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-37500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-36000] due to args.save_total_limit\n",
      " 38%|███▊      | 38000/100000 [2:52:06<4:45:27,  3.62it/s] Saving model checkpoint to model-bArt/checkpoint-38000\n",
      "Configuration saved in model-bArt/checkpoint-38000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0275, 'learning_rate': 1.2402800000000001e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-38000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-38000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-38000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-36500] due to args.save_total_limit\n",
      " 38%|███▊      | 38500/100000 [2:54:28<4:45:52,  3.59it/s] Saving model checkpoint to model-bArt/checkpoint-38500\n",
      "Configuration saved in model-bArt/checkpoint-38500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9704, 'learning_rate': 1.2302800000000002e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-38500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-38500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-38500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-37000] due to args.save_total_limit\n",
      " 39%|███▉      | 39000/100000 [2:56:51<4:38:57,  3.64it/s] Saving model checkpoint to model-bArt/checkpoint-39000\n",
      "Configuration saved in model-bArt/checkpoint-39000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9968, 'learning_rate': 1.2203e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-39000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-39000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-39000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-37500] due to args.save_total_limit\n",
      " 40%|███▉      | 39500/100000 [2:59:13<4:39:38,  3.61it/s] Saving model checkpoint to model-bArt/checkpoint-39500\n",
      "Configuration saved in model-bArt/checkpoint-39500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0154, 'learning_rate': 1.2103e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-39500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-39500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-39500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-38000] due to args.save_total_limit\n",
      " 40%|████      | 40000/100000 [3:01:33<7:01:19,  2.37it/s] Saving model checkpoint to model-bArt/checkpoint-40000\n",
      "Configuration saved in model-bArt/checkpoint-40000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9913, 'learning_rate': 1.2003e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-40000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-38500] due to args.save_total_limit\n",
      " 40%|████      | 40500/100000 [3:03:59<3:51:26,  4.28it/s] Saving model checkpoint to model-bArt/checkpoint-40500\n",
      "Configuration saved in model-bArt/checkpoint-40500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0151, 'learning_rate': 1.1903e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-40500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-40500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-40500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-39000] due to args.save_total_limit\n",
      " 41%|████      | 41000/100000 [3:06:01<3:48:11,  4.31it/s] Saving model checkpoint to model-bArt/checkpoint-41000\n",
      "Configuration saved in model-bArt/checkpoint-41000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.99, 'learning_rate': 1.1803e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-41000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-41000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-41000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-39500] due to args.save_total_limit\n",
      " 42%|████▏     | 41500/100000 [3:08:05<4:14:32,  3.83it/s] Saving model checkpoint to model-bArt/checkpoint-41500\n",
      "Configuration saved in model-bArt/checkpoint-41500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9803, 'learning_rate': 1.1703e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-41500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-41500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-41500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-40000] due to args.save_total_limit\n",
      " 42%|████▏     | 42000/100000 [3:10:16<3:51:06,  4.18it/s] Saving model checkpoint to model-bArt/checkpoint-42000\n",
      "Configuration saved in model-bArt/checkpoint-42000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9913, 'learning_rate': 1.1603000000000002e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-42000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-42000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-42000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-40500] due to args.save_total_limit\n",
      " 42%|████▎     | 42500/100000 [3:12:30<3:42:57,  4.30it/s] Saving model checkpoint to model-bArt/checkpoint-42500\n",
      "Configuration saved in model-bArt/checkpoint-42500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9903, 'learning_rate': 1.1503000000000002e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-42500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-42500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-42500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-41000] due to args.save_total_limit\n",
      " 43%|████▎     | 43000/100000 [3:14:29<3:43:49,  4.24it/s] Saving model checkpoint to model-bArt/checkpoint-43000\n",
      "Configuration saved in model-bArt/checkpoint-43000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9916, 'learning_rate': 1.1403000000000002e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-43000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-43000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-43000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-41500] due to args.save_total_limit\n",
      " 44%|████▎     | 43500/100000 [3:16:35<4:09:18,  3.78it/s] Saving model checkpoint to model-bArt/checkpoint-43500\n",
      "Configuration saved in model-bArt/checkpoint-43500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9851, 'learning_rate': 1.13032e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-43500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-43500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-43500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-42000] due to args.save_total_limit\n",
      " 44%|████▍     | 44000/100000 [3:18:51<4:13:08,  3.69it/s] Saving model checkpoint to model-bArt/checkpoint-44000\n",
      "Configuration saved in model-bArt/checkpoint-44000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9696, 'learning_rate': 1.1203200000000001e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-44000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-44000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-44000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-42500] due to args.save_total_limit\n",
      " 44%|████▍     | 44500/100000 [3:20:13<2:10:46,  7.07it/s] Saving model checkpoint to model-bArt/checkpoint-44500\n",
      "Configuration saved in model-bArt/checkpoint-44500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9884, 'learning_rate': 1.1103400000000001e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-44500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-44500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-44500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-43000] due to args.save_total_limit\n",
      " 45%|████▌     | 45000/100000 [3:21:27<2:07:34,  7.19it/s] Saving model checkpoint to model-bArt/checkpoint-45000\n",
      "Configuration saved in model-bArt/checkpoint-45000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9998, 'learning_rate': 1.1003400000000002e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-45000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-45000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-45000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-43500] due to args.save_total_limit\n",
      " 46%|████▌     | 45500/100000 [3:22:56<3:57:48,  3.82it/s] Saving model checkpoint to model-bArt/checkpoint-45500\n",
      "Configuration saved in model-bArt/checkpoint-45500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9756, 'learning_rate': 1.0903400000000001e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-45500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-45500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-45500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-44000] due to args.save_total_limit\n",
      " 46%|████▌     | 46000/100000 [3:25:12<3:49:47,  3.92it/s] Saving model checkpoint to model-bArt/checkpoint-46000\n",
      "Configuration saved in model-bArt/checkpoint-46000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.969, 'learning_rate': 1.0803400000000002e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-46000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-46000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-46000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-44500] due to args.save_total_limit\n",
      " 46%|████▋     | 46500/100000 [3:27:21<3:57:11,  3.76it/s] Saving model checkpoint to model-bArt/checkpoint-46500\n",
      "Configuration saved in model-bArt/checkpoint-46500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9694, 'learning_rate': 1.0703400000000002e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-46500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-46500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-46500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-45000] due to args.save_total_limit\n",
      " 47%|████▋     | 47000/100000 [3:29:25<3:21:37,  4.38it/s] Saving model checkpoint to model-bArt/checkpoint-47000\n",
      "Configuration saved in model-bArt/checkpoint-47000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.966, 'learning_rate': 1.06034e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-47000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-47000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-47000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-45500] due to args.save_total_limit\n",
      " 48%|████▊     | 47500/100000 [3:31:24<3:24:26,  4.28it/s] Saving model checkpoint to model-bArt/checkpoint-47500\n",
      "Configuration saved in model-bArt/checkpoint-47500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9758, 'learning_rate': 1.0503400000000002e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-47500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-47500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-47500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-46000] due to args.save_total_limit\n",
      " 48%|████▊     | 48000/100000 [3:33:23<3:16:27,  4.41it/s] Saving model checkpoint to model-bArt/checkpoint-48000\n",
      "Configuration saved in model-bArt/checkpoint-48000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0025, 'learning_rate': 1.0403400000000001e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-48000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-48000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-48000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-46500] due to args.save_total_limit\n",
      " 48%|████▊     | 48500/100000 [3:35:22<3:20:42,  4.28it/s] Saving model checkpoint to model-bArt/checkpoint-48500\n",
      "Configuration saved in model-bArt/checkpoint-48500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0088, 'learning_rate': 1.0303400000000002e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-48500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-48500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-48500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-47000] due to args.save_total_limit\n",
      " 49%|████▉     | 49000/100000 [3:37:21<3:15:12,  4.35it/s] Saving model checkpoint to model-bArt/checkpoint-49000\n",
      "Configuration saved in model-bArt/checkpoint-49000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.97, 'learning_rate': 1.0203400000000001e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-49000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-49000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-49000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-47500] due to args.save_total_limit\n",
      " 50%|████▉     | 49500/100000 [3:39:20<3:13:41,  4.35it/s] Saving model checkpoint to model-bArt/checkpoint-49500\n",
      "Configuration saved in model-bArt/checkpoint-49500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9587, 'learning_rate': 1.01034e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-49500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-49500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-49500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-48000] due to args.save_total_limit\n",
      " 50%|█████     | 50000/100000 [3:41:20<3:14:38,  4.28it/s] Saving model checkpoint to model-bArt/checkpoint-50000\n",
      "Configuration saved in model-bArt/checkpoint-50000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9358, 'learning_rate': 1.0003400000000001e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-50000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-50000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-50000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-48500] due to args.save_total_limit\n",
      " 50%|█████     | 50500/100000 [3:43:21<3:09:57,  4.34it/s] Saving model checkpoint to model-bArt/checkpoint-50500\n",
      "Configuration saved in model-bArt/checkpoint-50500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9802, 'learning_rate': 9.9034e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-50500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-50500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-50500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-49000] due to args.save_total_limit\n",
      " 51%|█████     | 51000/100000 [3:45:22<3:08:44,  4.33it/s] Saving model checkpoint to model-bArt/checkpoint-51000\n",
      "Configuration saved in model-bArt/checkpoint-51000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9745, 'learning_rate': 9.8034e-06, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-51000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-51000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-51000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-49500] due to args.save_total_limit\n",
      " 52%|█████▏    | 51500/100000 [3:47:22<3:11:21,  4.22it/s] Saving model checkpoint to model-bArt/checkpoint-51500\n",
      "Configuration saved in model-bArt/checkpoint-51500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9483, 'learning_rate': 9.703400000000001e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-51500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-51500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-51500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-50000] due to args.save_total_limit\n",
      " 52%|█████▏    | 52000/100000 [3:49:23<3:05:11,  4.32it/s] Saving model checkpoint to model-bArt/checkpoint-52000\n",
      "Configuration saved in model-bArt/checkpoint-52000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9596, 'learning_rate': 9.6036e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-52000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-52000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-52000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-50500] due to args.save_total_limit\n",
      " 52%|█████▎    | 52500/100000 [3:51:23<3:05:36,  4.27it/s] Saving model checkpoint to model-bArt/checkpoint-52500\n",
      "Configuration saved in model-bArt/checkpoint-52500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9489, 'learning_rate': 9.5036e-06, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-52500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-52500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-52500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-51000] due to args.save_total_limit\n",
      " 53%|█████▎    | 53000/100000 [3:53:24<3:03:19,  4.27it/s] Saving model checkpoint to model-bArt/checkpoint-53000\n",
      "Configuration saved in model-bArt/checkpoint-53000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9685, 'learning_rate': 9.403600000000001e-06, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-53000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-53000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-53000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-51500] due to args.save_total_limit\n",
      " 54%|█████▎    | 53500/100000 [3:55:24<3:01:21,  4.27it/s] Saving model checkpoint to model-bArt/checkpoint-53500\n",
      "Configuration saved in model-bArt/checkpoint-53500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9405, 'learning_rate': 9.3036e-06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-53500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-53500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-53500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-52000] due to args.save_total_limit\n",
      " 54%|█████▍    | 54000/100000 [3:57:25<2:58:03,  4.31it/s] Saving model checkpoint to model-bArt/checkpoint-54000\n",
      "Configuration saved in model-bArt/checkpoint-54000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9859, 'learning_rate': 9.203600000000001e-06, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-54000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-54000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-54000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-52500] due to args.save_total_limit\n",
      " 55%|█████▍    | 54500/100000 [3:59:25<3:01:48,  4.17it/s] Saving model checkpoint to model-bArt/checkpoint-54500\n",
      "Configuration saved in model-bArt/checkpoint-54500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9907, 'learning_rate': 9.1036e-06, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-54500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-54500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-54500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-53000] due to args.save_total_limit\n",
      " 55%|█████▌    | 55000/100000 [4:01:25<2:56:24,  4.25it/s] Saving model checkpoint to model-bArt/checkpoint-55000\n",
      "Configuration saved in model-bArt/checkpoint-55000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9691, 'learning_rate': 9.004200000000001e-06, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-55000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-55000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-55000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-53500] due to args.save_total_limit\n",
      " 56%|█████▌    | 55500/100000 [4:03:26<2:59:13,  4.14it/s] Saving model checkpoint to model-bArt/checkpoint-55500\n",
      "Configuration saved in model-bArt/checkpoint-55500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.971, 'learning_rate': 8.9042e-06, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-55500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-55500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-55500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-54000] due to args.save_total_limit\n",
      " 56%|█████▌    | 56000/100000 [4:05:26<2:51:08,  4.29it/s] Saving model checkpoint to model-bArt/checkpoint-56000\n",
      "Configuration saved in model-bArt/checkpoint-56000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9498, 'learning_rate': 8.804200000000001e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-56000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-56000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-56000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-54500] due to args.save_total_limit\n",
      " 56%|█████▋    | 56500/100000 [4:07:27<2:50:28,  4.25it/s] Saving model checkpoint to model-bArt/checkpoint-56500\n",
      "Configuration saved in model-bArt/checkpoint-56500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9496, 'learning_rate': 8.7042e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-56500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-56500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-56500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-55000] due to args.save_total_limit\n",
      " 57%|█████▋    | 57000/100000 [4:09:28<2:47:00,  4.29it/s] Saving model checkpoint to model-bArt/checkpoint-57000\n",
      "Configuration saved in model-bArt/checkpoint-57000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.947, 'learning_rate': 8.6042e-06, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-57000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-57000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-57000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-55500] due to args.save_total_limit\n",
      " 57%|█████▊    | 57500/100000 [4:11:28<2:49:50,  4.17it/s] Saving model checkpoint to model-bArt/checkpoint-57500\n",
      "Configuration saved in model-bArt/checkpoint-57500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9415, 'learning_rate': 8.5042e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-57500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-57500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-57500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-56000] due to args.save_total_limit\n",
      " 58%|█████▊    | 58000/100000 [4:13:28<2:40:55,  4.35it/s] Saving model checkpoint to model-bArt/checkpoint-58000\n",
      "Configuration saved in model-bArt/checkpoint-58000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9541, 'learning_rate': 8.4042e-06, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-58000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-58000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-58000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-56500] due to args.save_total_limit\n",
      " 58%|█████▊    | 58500/100000 [4:15:29<2:43:40,  4.23it/s] Saving model checkpoint to model-bArt/checkpoint-58500\n",
      "Configuration saved in model-bArt/checkpoint-58500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9617, 'learning_rate': 8.304200000000001e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-58500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-58500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-58500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-57000] due to args.save_total_limit\n",
      " 59%|█████▉    | 59000/100000 [4:17:29<2:38:29,  4.31it/s] Saving model checkpoint to model-bArt/checkpoint-59000\n",
      "Configuration saved in model-bArt/checkpoint-59000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9485, 'learning_rate': 8.204200000000002e-06, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-59000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-59000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-59000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-57500] due to args.save_total_limit\n",
      " 60%|█████▉    | 59500/100000 [4:19:30<2:37:21,  4.29it/s] Saving model checkpoint to model-bArt/checkpoint-59500\n",
      "Configuration saved in model-bArt/checkpoint-59500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9343, 'learning_rate': 8.104200000000001e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-59500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-59500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-59500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-58000] due to args.save_total_limit\n",
      " 60%|██████    | 60000/100000 [4:21:30<2:38:17,  4.21it/s] Saving model checkpoint to model-bArt/checkpoint-60000\n",
      "Configuration saved in model-bArt/checkpoint-60000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9688, 'learning_rate': 8.0042e-06, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-60000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-58500] due to args.save_total_limit\n",
      " 60%|██████    | 60500/100000 [4:23:31<2:34:30,  4.26it/s] Saving model checkpoint to model-bArt/checkpoint-60500\n",
      "Configuration saved in model-bArt/checkpoint-60500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9614, 'learning_rate': 7.904200000000001e-06, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-60500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-60500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-60500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-59000] due to args.save_total_limit\n",
      " 61%|██████    | 61000/100000 [4:25:32<2:31:13,  4.30it/s] Saving model checkpoint to model-bArt/checkpoint-61000\n",
      "Configuration saved in model-bArt/checkpoint-61000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9704, 'learning_rate': 7.8042e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-61000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-61000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-61000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-59500] due to args.save_total_limit\n",
      " 62%|██████▏   | 61500/100000 [4:27:33<2:31:26,  4.24it/s] Saving model checkpoint to model-bArt/checkpoint-61500\n",
      "Configuration saved in model-bArt/checkpoint-61500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9482, 'learning_rate': 7.7042e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-61500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-61500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-61500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-60000] due to args.save_total_limit\n",
      " 62%|██████▏   | 62000/100000 [4:29:34<2:29:48,  4.23it/s] Saving model checkpoint to model-bArt/checkpoint-62000\n",
      "Configuration saved in model-bArt/checkpoint-62000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9638, 'learning_rate': 7.604200000000001e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-62000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-62000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-62000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-60500] due to args.save_total_limit\n",
      " 62%|██████▎   | 62500/100000 [4:31:35<2:31:16,  4.13it/s] Saving model checkpoint to model-bArt/checkpoint-62500\n",
      "Configuration saved in model-bArt/checkpoint-62500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9386, 'learning_rate': 7.5042e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-62500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-62500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-62500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-61000] due to args.save_total_limit\n",
      " 63%|██████▎   | 63000/100000 [4:33:35<2:21:37,  4.35it/s] Saving model checkpoint to model-bArt/checkpoint-63000\n",
      "Configuration saved in model-bArt/checkpoint-63000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9603, 'learning_rate': 7.404600000000001e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-63000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-63000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-63000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-61500] due to args.save_total_limit\n",
      " 64%|██████▎   | 63500/100000 [4:35:35<2:21:09,  4.31it/s] Saving model checkpoint to model-bArt/checkpoint-63500\n",
      "Configuration saved in model-bArt/checkpoint-63500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9505, 'learning_rate': 7.3046e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-63500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-63500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-63500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-62000] due to args.save_total_limit\n",
      " 64%|██████▍   | 64000/100000 [4:37:36<2:18:36,  4.33it/s] Saving model checkpoint to model-bArt/checkpoint-64000\n",
      "Configuration saved in model-bArt/checkpoint-64000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9488, 'learning_rate': 7.2046000000000004e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-64000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-64000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-64000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-62500] due to args.save_total_limit\n",
      " 64%|██████▍   | 64500/100000 [4:39:37<2:19:12,  4.25it/s] Saving model checkpoint to model-bArt/checkpoint-64500\n",
      "Configuration saved in model-bArt/checkpoint-64500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9429, 'learning_rate': 7.1046000000000006e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-64500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-64500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-64500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-63000] due to args.save_total_limit\n",
      " 65%|██████▌   | 65000/100000 [4:41:37<2:17:24,  4.25it/s] Saving model checkpoint to model-bArt/checkpoint-65000\n",
      "Configuration saved in model-bArt/checkpoint-65000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.96, 'learning_rate': 7.004600000000001e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-65000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-65000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-65000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-63500] due to args.save_total_limit\n",
      " 66%|██████▌   | 65500/100000 [4:43:38<2:12:38,  4.34it/s] Saving model checkpoint to model-bArt/checkpoint-65500\n",
      "Configuration saved in model-bArt/checkpoint-65500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9567, 'learning_rate': 6.9046e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-65500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-65500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-65500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-64000] due to args.save_total_limit\n",
      " 66%|██████▌   | 66000/100000 [4:45:38<2:11:07,  4.32it/s] Saving model checkpoint to model-bArt/checkpoint-66000\n",
      "Configuration saved in model-bArt/checkpoint-66000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9483, 'learning_rate': 6.8048000000000005e-06, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-66000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-66000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-66000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-64500] due to args.save_total_limit\n",
      " 66%|██████▋   | 66500/100000 [4:47:39<2:12:40,  4.21it/s] Saving model checkpoint to model-bArt/checkpoint-66500\n",
      "Configuration saved in model-bArt/checkpoint-66500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9319, 'learning_rate': 6.704800000000001e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-66500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-66500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-66500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-65000] due to args.save_total_limit\n",
      " 67%|██████▋   | 67000/100000 [4:49:39<2:07:58,  4.30it/s] Saving model checkpoint to model-bArt/checkpoint-67000\n",
      "Configuration saved in model-bArt/checkpoint-67000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9574, 'learning_rate': 6.605e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-67000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-67000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-67000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-65500] due to args.save_total_limit\n",
      " 68%|██████▊   | 67500/100000 [4:51:40<2:08:21,  4.22it/s] Saving model checkpoint to model-bArt/checkpoint-67500\n",
      "Configuration saved in model-bArt/checkpoint-67500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9448, 'learning_rate': 6.505e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-67500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-67500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-67500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-66000] due to args.save_total_limit\n",
      " 68%|██████▊   | 68000/100000 [4:53:41<2:03:25,  4.32it/s] Saving model checkpoint to model-bArt/checkpoint-68000\n",
      "Configuration saved in model-bArt/checkpoint-68000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9128, 'learning_rate': 6.4050000000000005e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-68000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-68000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-68000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-66500] due to args.save_total_limit\n",
      " 68%|██████▊   | 68500/100000 [4:55:42<2:03:12,  4.26it/s] Saving model checkpoint to model-bArt/checkpoint-68500\n",
      "Configuration saved in model-bArt/checkpoint-68500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9268, 'learning_rate': 6.305e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-68500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-68500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-68500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-67000] due to args.save_total_limit\n",
      " 69%|██████▉   | 69000/100000 [4:57:42<2:04:15,  4.16it/s]Saving model checkpoint to model-bArt/checkpoint-69000\n",
      "Configuration saved in model-bArt/checkpoint-69000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9086, 'learning_rate': 6.205000000000001e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-69000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-69000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-69000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-67500] due to args.save_total_limit\n",
      " 70%|██████▉   | 69500/100000 [4:59:43<1:58:18,  4.30it/s]Saving model checkpoint to model-bArt/checkpoint-69500\n",
      "Configuration saved in model-bArt/checkpoint-69500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9634, 'learning_rate': 6.105000000000001e-06, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-69500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-69500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-69500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-68000] due to args.save_total_limit\n",
      " 70%|███████   | 70000/100000 [5:01:43<1:57:21,  4.26it/s]Saving model checkpoint to model-bArt/checkpoint-70000\n",
      "Configuration saved in model-bArt/checkpoint-70000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.918, 'learning_rate': 6.005000000000001e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-70000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-70000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-70000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-68500] due to args.save_total_limit\n",
      " 70%|███████   | 70500/100000 [5:03:44<1:53:27,  4.33it/s]Saving model checkpoint to model-bArt/checkpoint-70500\n",
      "Configuration saved in model-bArt/checkpoint-70500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9498, 'learning_rate': 5.905000000000001e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-70500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-70500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-70500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-69000] due to args.save_total_limit\n",
      " 71%|███████   | 71000/100000 [5:05:44<1:53:26,  4.26it/s]Saving model checkpoint to model-bArt/checkpoint-71000\n",
      "Configuration saved in model-bArt/checkpoint-71000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9287, 'learning_rate': 5.805e-06, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-71000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-71000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-71000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-69500] due to args.save_total_limit\n",
      " 72%|███████▏  | 71500/100000 [5:07:44<1:48:20,  4.38it/s]Saving model checkpoint to model-bArt/checkpoint-71500\n",
      "Configuration saved in model-bArt/checkpoint-71500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9405, 'learning_rate': 5.7050000000000004e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-71500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-71500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-71500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-70000] due to args.save_total_limit\n",
      " 72%|███████▏  | 71779/100000 [5:08:53<1:38:45,  4.76it/s]The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, article, highlights.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 4\n",
      "                                                          \n",
      " 72%|███████▏  | 71779/100000 [5:12:24<1:38:45,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7094402313232422, 'eval_runtime': 211.3275, 'eval_samples_per_second': 63.257, 'eval_steps_per_second': 15.814, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72000/100000 [5:13:16<1:47:49,  4.33it/s]  Saving model checkpoint to model-bArt/checkpoint-72000\n",
      "Configuration saved in model-bArt/checkpoint-72000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8667, 'learning_rate': 5.6050000000000005e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-72000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-72000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-72000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-70500] due to args.save_total_limit\n",
      " 72%|███████▎  | 72500/100000 [5:15:16<1:49:22,  4.19it/s]Saving model checkpoint to model-bArt/checkpoint-72500\n",
      "Configuration saved in model-bArt/checkpoint-72500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.848, 'learning_rate': 5.505000000000001e-06, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-72500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-72500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-72500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-71000] due to args.save_total_limit\n",
      " 73%|███████▎  | 73000/100000 [5:17:17<1:45:15,  4.27it/s]Saving model checkpoint to model-bArt/checkpoint-73000\n",
      "Configuration saved in model-bArt/checkpoint-73000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8619, 'learning_rate': 5.405e-06, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-73000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-73000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-73000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-71500] due to args.save_total_limit\n",
      " 74%|███████▎  | 73500/100000 [5:19:17<1:44:49,  4.21it/s]Saving model checkpoint to model-bArt/checkpoint-73500\n",
      "Configuration saved in model-bArt/checkpoint-73500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8558, 'learning_rate': 5.305e-06, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-73500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-73500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-73500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-72000] due to args.save_total_limit\n",
      " 74%|███████▍  | 74000/100000 [5:21:18<1:42:07,  4.24it/s]Saving model checkpoint to model-bArt/checkpoint-74000\n",
      "Configuration saved in model-bArt/checkpoint-74000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8753, 'learning_rate': 5.205e-06, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-74000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-74000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-74000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-72500] due to args.save_total_limit\n",
      " 74%|███████▍  | 74500/100000 [5:23:18<1:39:25,  4.27it/s]Saving model checkpoint to model-bArt/checkpoint-74500\n",
      "Configuration saved in model-bArt/checkpoint-74500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8537, 'learning_rate': 5.105e-06, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-74500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-74500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-74500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-73000] due to args.save_total_limit\n",
      " 75%|███████▌  | 75000/100000 [5:25:18<1:37:46,  4.26it/s]Saving model checkpoint to model-bArt/checkpoint-75000\n",
      "Configuration saved in model-bArt/checkpoint-75000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8491, 'learning_rate': 5.0049999999999995e-06, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-75000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-75000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-75000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-73500] due to args.save_total_limit\n",
      " 76%|███████▌  | 75500/100000 [5:27:19<1:36:04,  4.25it/s]Saving model checkpoint to model-bArt/checkpoint-75500\n",
      "Configuration saved in model-bArt/checkpoint-75500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8497, 'learning_rate': 4.9054000000000005e-06, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-75500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-75500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-75500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-74000] due to args.save_total_limit\n",
      " 76%|███████▌  | 76000/100000 [5:29:19<1:34:54,  4.21it/s]Saving model checkpoint to model-bArt/checkpoint-76000\n",
      "Configuration saved in model-bArt/checkpoint-76000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8756, 'learning_rate': 4.805400000000001e-06, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-76000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-76000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-76000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-74500] due to args.save_total_limit\n",
      " 76%|███████▋  | 76500/100000 [5:31:20<1:33:31,  4.19it/s]Saving model checkpoint to model-bArt/checkpoint-76500\n",
      "Configuration saved in model-bArt/checkpoint-76500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8772, 'learning_rate': 4.705400000000001e-06, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-76500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-76500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-76500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-75000] due to args.save_total_limit\n",
      " 77%|███████▋  | 77000/100000 [5:33:21<1:31:02,  4.21it/s]Saving model checkpoint to model-bArt/checkpoint-77000\n",
      "Configuration saved in model-bArt/checkpoint-77000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8699, 'learning_rate': 4.605400000000001e-06, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-77000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-77000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-77000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-75500] due to args.save_total_limit\n",
      " 78%|███████▊  | 77500/100000 [5:35:21<1:28:10,  4.25it/s]Saving model checkpoint to model-bArt/checkpoint-77500\n",
      "Configuration saved in model-bArt/checkpoint-77500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8641, 'learning_rate': 4.5054e-06, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-77500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-77500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-77500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-76000] due to args.save_total_limit\n",
      " 78%|███████▊  | 78000/100000 [5:37:22<1:26:31,  4.24it/s]Saving model checkpoint to model-bArt/checkpoint-78000\n",
      "Configuration saved in model-bArt/checkpoint-78000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8397, 'learning_rate': 4.4054e-06, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-78000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-78000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-78000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-76500] due to args.save_total_limit\n",
      " 78%|███████▊  | 78500/100000 [5:39:23<1:25:28,  4.19it/s]Saving model checkpoint to model-bArt/checkpoint-78500\n",
      "Configuration saved in model-bArt/checkpoint-78500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.855, 'learning_rate': 4.3054e-06, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-78500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-78500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-78500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-77000] due to args.save_total_limit\n",
      " 79%|███████▉  | 79000/100000 [5:41:24<1:23:17,  4.20it/s]Saving model checkpoint to model-bArt/checkpoint-79000\n",
      "Configuration saved in model-bArt/checkpoint-79000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8071, 'learning_rate': 4.2054000000000004e-06, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-79000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-79000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-79000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-77500] due to args.save_total_limit\n",
      " 80%|███████▉  | 79500/100000 [5:43:24<1:19:13,  4.31it/s]Saving model checkpoint to model-bArt/checkpoint-79500\n",
      "Configuration saved in model-bArt/checkpoint-79500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8658, 'learning_rate': 4.1056e-06, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-79500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-79500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-79500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-78000] due to args.save_total_limit\n",
      " 80%|████████  | 80000/100000 [5:45:25<1:16:58,  4.33it/s]Saving model checkpoint to model-bArt/checkpoint-80000\n",
      "Configuration saved in model-bArt/checkpoint-80000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.866, 'learning_rate': 4.0056e-06, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-80000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-80000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-80000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-78500] due to args.save_total_limit\n",
      " 80%|████████  | 80500/100000 [5:47:26<1:16:37,  4.24it/s]Saving model checkpoint to model-bArt/checkpoint-80500\n",
      "Configuration saved in model-bArt/checkpoint-80500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8545, 'learning_rate': 3.9056e-06, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-80500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-80500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-80500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-79000] due to args.save_total_limit\n",
      " 81%|████████  | 81000/100000 [5:49:26<1:14:00,  4.28it/s]Saving model checkpoint to model-bArt/checkpoint-81000\n",
      "Configuration saved in model-bArt/checkpoint-81000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8332, 'learning_rate': 3.8056000000000005e-06, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-81000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-81000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-81000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-79500] due to args.save_total_limit\n",
      " 82%|████████▏ | 81500/100000 [5:51:27<1:13:20,  4.20it/s]Saving model checkpoint to model-bArt/checkpoint-81500\n",
      "Configuration saved in model-bArt/checkpoint-81500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8493, 'learning_rate': 3.7058000000000006e-06, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-81500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-81500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-81500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-80000] due to args.save_total_limit\n",
      " 82%|████████▏ | 82000/100000 [5:53:28<1:10:56,  4.23it/s]Saving model checkpoint to model-bArt/checkpoint-82000\n",
      "Configuration saved in model-bArt/checkpoint-82000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8248, 'learning_rate': 3.6058000000000003e-06, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-82000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-82000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-82000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-80500] due to args.save_total_limit\n",
      " 82%|████████▎ | 82500/100000 [5:55:29<1:07:50,  4.30it/s]Saving model checkpoint to model-bArt/checkpoint-82500\n",
      "Configuration saved in model-bArt/checkpoint-82500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8356, 'learning_rate': 3.5058000000000004e-06, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-82500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-82500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-82500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-81000] due to args.save_total_limit\n",
      " 83%|████████▎ | 83000/100000 [5:57:29<1:05:17,  4.34it/s]Saving model checkpoint to model-bArt/checkpoint-83000\n",
      "Configuration saved in model-bArt/checkpoint-83000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8466, 'learning_rate': 3.4058e-06, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-83000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-83000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-83000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-81500] due to args.save_total_limit\n",
      " 84%|████████▎ | 83500/100000 [5:59:30<1:03:12,  4.35it/s]Saving model checkpoint to model-bArt/checkpoint-83500\n",
      "Configuration saved in model-bArt/checkpoint-83500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8249, 'learning_rate': 3.306e-06, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-83500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-83500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-83500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-82000] due to args.save_total_limit\n",
      " 84%|████████▍ | 84000/100000 [6:01:31<1:03:24,  4.21it/s]Saving model checkpoint to model-bArt/checkpoint-84000\n",
      "Configuration saved in model-bArt/checkpoint-84000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8228, 'learning_rate': 3.2060000000000003e-06, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-84000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-84000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-84000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-82500] due to args.save_total_limit\n",
      " 84%|████████▍ | 84500/100000 [6:03:31<1:00:44,  4.25it/s]Saving model checkpoint to model-bArt/checkpoint-84500\n",
      "Configuration saved in model-bArt/checkpoint-84500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8525, 'learning_rate': 3.106e-06, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-84500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-84500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-84500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-83000] due to args.save_total_limit\n",
      " 85%|████████▌ | 85000/100000 [6:05:32<57:19,  4.36it/s]  Saving model checkpoint to model-bArt/checkpoint-85000\n",
      "Configuration saved in model-bArt/checkpoint-85000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8692, 'learning_rate': 3.006e-06, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-85000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-85000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-85000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-83500] due to args.save_total_limit\n",
      " 86%|████████▌ | 85500/100000 [6:07:32<56:33,  4.27it/s]  Saving model checkpoint to model-bArt/checkpoint-85500\n",
      "Configuration saved in model-bArt/checkpoint-85500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8508, 'learning_rate': 2.9060000000000006e-06, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-85500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-85500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-85500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-84000] due to args.save_total_limit\n",
      " 86%|████████▌ | 86000/100000 [6:09:33<55:28,  4.21it/s]  Saving model checkpoint to model-bArt/checkpoint-86000\n",
      "Configuration saved in model-bArt/checkpoint-86000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8243, 'learning_rate': 2.8062e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-86000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-86000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-86000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-84500] due to args.save_total_limit\n",
      " 86%|████████▋ | 86500/100000 [6:11:34<53:16,  4.22it/s]  Saving model checkpoint to model-bArt/checkpoint-86500\n",
      "Configuration saved in model-bArt/checkpoint-86500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8541, 'learning_rate': 2.7062000000000004e-06, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-86500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-86500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-86500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-85000] due to args.save_total_limit\n",
      " 87%|████████▋ | 87000/100000 [6:13:35<50:23,  4.30it/s]  Saving model checkpoint to model-bArt/checkpoint-87000\n",
      "Configuration saved in model-bArt/checkpoint-87000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8129, 'learning_rate': 2.6062000000000006e-06, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-87000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-87000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-87000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-85500] due to args.save_total_limit\n",
      " 88%|████████▊ | 87500/100000 [6:15:35<48:37,  4.28it/s]  Saving model checkpoint to model-bArt/checkpoint-87500\n",
      "Configuration saved in model-bArt/checkpoint-87500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8507, 'learning_rate': 2.5062000000000002e-06, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-87500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-87500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-87500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-86000] due to args.save_total_limit\n",
      " 88%|████████▊ | 88000/100000 [6:17:36<47:17,  4.23it/s]  Saving model checkpoint to model-bArt/checkpoint-88000\n",
      "Configuration saved in model-bArt/checkpoint-88000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8563, 'learning_rate': 2.4062000000000004e-06, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-88000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-88000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-88000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-86500] due to args.save_total_limit\n",
      " 88%|████████▊ | 88500/100000 [6:19:37<46:07,  4.15it/s]  Saving model checkpoint to model-bArt/checkpoint-88500\n",
      "Configuration saved in model-bArt/checkpoint-88500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8228, 'learning_rate': 2.3062e-06, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-88500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-88500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-88500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-87000] due to args.save_total_limit\n",
      " 89%|████████▉ | 89000/100000 [6:21:37<44:15,  4.14it/s]  Saving model checkpoint to model-bArt/checkpoint-89000\n",
      "Configuration saved in model-bArt/checkpoint-89000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8698, 'learning_rate': 2.2062e-06, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-89000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-89000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-89000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-87500] due to args.save_total_limit\n",
      " 90%|████████▉ | 89500/100000 [6:23:38<42:12,  4.15it/s]  Saving model checkpoint to model-bArt/checkpoint-89500\n",
      "Configuration saved in model-bArt/checkpoint-89500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8333, 'learning_rate': 2.1062000000000003e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-89500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-89500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-89500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-88000] due to args.save_total_limit\n",
      " 90%|█████████ | 90000/100000 [6:25:39<38:58,  4.28it/s]  Saving model checkpoint to model-bArt/checkpoint-90000\n",
      "Configuration saved in model-bArt/checkpoint-90000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8357, 'learning_rate': 2.0064000000000004e-06, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-90000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-90000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-90000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-88500] due to args.save_total_limit\n",
      " 90%|█████████ | 90500/100000 [6:27:39<36:30,  4.34it/s]  Saving model checkpoint to model-bArt/checkpoint-90500\n",
      "Configuration saved in model-bArt/checkpoint-90500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8331, 'learning_rate': 1.9064000000000003e-06, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-90500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-90500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-90500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-89000] due to args.save_total_limit\n",
      " 91%|█████████ | 91000/100000 [6:29:39<34:43,  4.32it/s]  Saving model checkpoint to model-bArt/checkpoint-91000\n",
      "Configuration saved in model-bArt/checkpoint-91000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8502, 'learning_rate': 1.8066e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-91000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-91000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-91000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-89500] due to args.save_total_limit\n",
      " 92%|█████████▏| 91500/100000 [6:31:40<32:54,  4.30it/s]  Saving model checkpoint to model-bArt/checkpoint-91500\n",
      "Configuration saved in model-bArt/checkpoint-91500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8498, 'learning_rate': 1.7066e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-91500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-91500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-91500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-90000] due to args.save_total_limit\n",
      " 92%|█████████▏| 92000/100000 [6:33:40<30:46,  4.33it/s]  Saving model checkpoint to model-bArt/checkpoint-92000\n",
      "Configuration saved in model-bArt/checkpoint-92000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.828, 'learning_rate': 1.6066e-06, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-92000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-92000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-92000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-90500] due to args.save_total_limit\n",
      " 92%|█████████▎| 92500/100000 [6:35:41<29:18,  4.26it/s]  Saving model checkpoint to model-bArt/checkpoint-92500\n",
      "Configuration saved in model-bArt/checkpoint-92500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8343, 'learning_rate': 1.5066000000000001e-06, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-92500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-92500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-92500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-91000] due to args.save_total_limit\n",
      " 93%|█████████▎| 93000/100000 [6:37:41<26:49,  4.35it/s]  Saving model checkpoint to model-bArt/checkpoint-93000\n",
      "Configuration saved in model-bArt/checkpoint-93000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8237, 'learning_rate': 1.4066000000000002e-06, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-93000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-93000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-93000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-91500] due to args.save_total_limit\n",
      " 94%|█████████▎| 93500/100000 [6:39:42<25:40,  4.22it/s]  Saving model checkpoint to model-bArt/checkpoint-93500\n",
      "Configuration saved in model-bArt/checkpoint-93500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8298, 'learning_rate': 1.3066000000000001e-06, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-93500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-93500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-93500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-92000] due to args.save_total_limit\n",
      " 94%|█████████▍| 94000/100000 [6:41:42<23:35,  4.24it/s]  Saving model checkpoint to model-bArt/checkpoint-94000\n",
      "Configuration saved in model-bArt/checkpoint-94000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8508, 'learning_rate': 1.2066000000000002e-06, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-94000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-94000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-94000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-92500] due to args.save_total_limit\n",
      " 94%|█████████▍| 94500/100000 [6:43:42<21:52,  4.19it/s]  Saving model checkpoint to model-bArt/checkpoint-94500\n",
      "Configuration saved in model-bArt/checkpoint-94500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8641, 'learning_rate': 1.1068000000000001e-06, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-94500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-94500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-94500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-93000] due to args.save_total_limit\n",
      " 95%|█████████▌| 95000/100000 [6:45:43<19:19,  4.31it/s]  Saving model checkpoint to model-bArt/checkpoint-95000\n",
      "Configuration saved in model-bArt/checkpoint-95000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8573, 'learning_rate': 1.0068e-06, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-95000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-95000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-95000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-93500] due to args.save_total_limit\n",
      " 96%|█████████▌| 95500/100000 [6:47:43<17:33,  4.27it/s]  Saving model checkpoint to model-bArt/checkpoint-95500\n",
      "Configuration saved in model-bArt/checkpoint-95500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8481, 'learning_rate': 9.068e-07, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-95500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-95500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-95500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-94000] due to args.save_total_limit\n",
      " 96%|█████████▌| 96000/100000 [6:49:44<15:39,  4.26it/s]  Saving model checkpoint to model-bArt/checkpoint-96000\n",
      "Configuration saved in model-bArt/checkpoint-96000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8355, 'learning_rate': 8.068e-07, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-96000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-96000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-96000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-94500] due to args.save_total_limit\n",
      " 96%|█████████▋| 96500/100000 [6:51:45<13:50,  4.21it/s]  Saving model checkpoint to model-bArt/checkpoint-96500\n",
      "Configuration saved in model-bArt/checkpoint-96500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8542, 'learning_rate': 7.068000000000002e-07, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-96500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-96500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-96500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-95000] due to args.save_total_limit\n",
      " 97%|█████████▋| 97000/100000 [6:53:45<11:34,  4.32it/s]  Saving model checkpoint to model-bArt/checkpoint-97000\n",
      "Configuration saved in model-bArt/checkpoint-97000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8291, 'learning_rate': 6.068000000000001e-07, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-97000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-97000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-97000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-95500] due to args.save_total_limit\n",
      " 98%|█████████▊| 97500/100000 [6:55:46<09:43,  4.29it/s]Saving model checkpoint to model-bArt/checkpoint-97500\n",
      "Configuration saved in model-bArt/checkpoint-97500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8395, 'learning_rate': 5.068000000000001e-07, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-97500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-97500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-97500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-96000] due to args.save_total_limit\n",
      " 98%|█████████▊| 98000/100000 [6:57:47<08:02,  4.14it/s]Saving model checkpoint to model-bArt/checkpoint-98000\n",
      "Configuration saved in model-bArt/checkpoint-98000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8462, 'learning_rate': 4.068e-07, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-98000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-98000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-98000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-96500] due to args.save_total_limit\n",
      " 98%|█████████▊| 98500/100000 [6:59:47<05:59,  4.18it/s]Saving model checkpoint to model-bArt/checkpoint-98500\n",
      "Configuration saved in model-bArt/checkpoint-98500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8318, 'learning_rate': 3.0700000000000004e-07, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-98500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-98500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-98500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-97000] due to args.save_total_limit\n",
      " 99%|█████████▉| 99000/100000 [7:01:48<03:54,  4.26it/s]Saving model checkpoint to model-bArt/checkpoint-99000\n",
      "Configuration saved in model-bArt/checkpoint-99000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8362, 'learning_rate': 2.0700000000000001e-07, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-99000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-99000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-99000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-97500] due to args.save_total_limit\n",
      "100%|█████████▉| 99500/100000 [7:03:48<01:53,  4.41it/s]Saving model checkpoint to model-bArt/checkpoint-99500\n",
      "Configuration saved in model-bArt/checkpoint-99500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8295, 'learning_rate': 1.07e-07, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-99500\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-99500\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-99500\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-98000] due to args.save_total_limit\n",
      "100%|██████████| 100000/100000 [7:05:48<00:00,  4.37it/s]Saving model checkpoint to model-bArt/checkpoint-100000\n",
      "Configuration saved in model-bArt/checkpoint-100000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8235, 'learning_rate': 7.200000000000001e-09, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model-bArt/checkpoint-100000\\pytorch_model.bin\n",
      "tokenizer config file saved in model-bArt/checkpoint-100000\\tokenizer_config.json\n",
      "Special tokens file saved in model-bArt/checkpoint-100000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model-bArt\\checkpoint-98500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: id, article, highlights.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 13368\n",
      "  Batch size = 4\n",
      "                                                         \n",
      "100%|██████████| 100000/100000 [7:09:23<00:00,  4.37it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 100000/100000 [7:09:23<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6965572834014893, 'eval_runtime': 211.4826, 'eval_samples_per_second': 63.211, 'eval_steps_per_second': 15.803, 'epoch': 1.39}\n",
      "{'train_runtime': 25763.1468, 'train_samples_per_second': 15.526, 'train_steps_per_second': 3.882, 'train_loss': 1.9768753704833983, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100000, training_loss=1.9768753704833983, metrics={'train_runtime': 25763.1468, 'train_samples_per_second': 15.526, 'train_steps_per_second': 3.882, 'train_loss': 1.9768753704833983, 'epoch': 1.39})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188ff91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model-bart/checkpoint-100000\\config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model-bart/checkpoint-100000\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at model-bart/checkpoint-100000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "100%|██████████| 2873/2873 [52:42<00:00,  1.10s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=0.4605874615539056, recall=0.33554514155456583, fmeasure=0.3774961097112488), mid=Score(precision=0.4636644874498747, recall=0.3378481355460129, fmeasure=0.37973542885550016), high=Score(precision=0.46660590220384457, recall=0.3402144520967871, fmeasure=0.3819739565360807)), 'rouge2': AggregateScore(low=Score(precision=0.2172959171587987, recall=0.15582968428055113, fmeasure=0.17620488751991992), mid=Score(precision=0.22046391619136546, recall=0.1580862317138214, fmeasure=0.17864458420103718), high=Score(precision=0.22343211540201685, recall=0.16025704795134635, fmeasure=0.18097848938380556)), 'rougeL': AggregateScore(low=Score(precision=0.33396711283347935, recall=0.24434611218828406, fmeasure=0.2741989224052487), mid=Score(precision=0.3368144273288295, recall=0.2466339912211179, fmeasure=0.27648972026284024), high=Score(precision=0.3397218152858248, recall=0.2487447917699668, fmeasure=0.27888171733163847)), 'rougeLsum': AggregateScore(low=Score(precision=0.43071745559677843, recall=0.3138496916066121, fmeasure=0.3529894015009587), mid=Score(precision=0.4338497045181547, recall=0.3161539917973527, fmeasure=0.355384803266528), high=Score(precision=0.4370941913743803, recall=0.31838975598115377, fmeasure=0.35762567499324144))}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('model-bart/checkpoint-100000')\n",
    "model.to('cuda')\n",
    "\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "xtest = raw_datasets['test']\n",
    "#xtest = xtest.select(range(100))\n",
    "\n",
    "min_generated_length = 25\n",
    "max_generated_length = 75\n",
    "\n",
    "def generate_summary(batch):\n",
    "    inputs = tokenizer(batch[\"article\"], padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs.input_ids.to('cuda'), \n",
    "                             attention_mask=inputs.attention_mask.to(\"cuda\"), \n",
    "                             min_length=min_generated_length, \n",
    "                             max_length=max_generated_length, \n",
    "                             length_penalty=3.0)\n",
    "\n",
    "    batch[\"pred\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return batch\n",
    "\n",
    "res = xtest.map(generate_summary, batched=True, batch_size=4, remove_columns=['article'])\n",
    "reckful = rouge.compute(predictions=res['pred'], references=res['highlights'])\n",
    "print(reckful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5666c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlight:  \n",
      " \n",
      " James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88 .\n",
      "\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " \"The Dukes of Hazzard\" star James Best dies at 88.\n",
      "Best played sheriff Rosco P. Coltrane on TV show.\n",
      "He was best known for his role in \"The Twilight Zone,\" \"Bon \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " A lawyer for Dr. Anthony Moschetto says the charges against him are baseless .\n",
      "Moschetto, 54, was arrested for selling drugs and weapons, prosecutors say .\n",
      "Authorities allege Moschetto hired accomplices to burn down the practice of former associate . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " NEW: \"None of anything in this case has any evidentiary value,\" attorney says.\n",
      "Dr. Anthony Moschetto, 54, pleaded not guilty to all charges Wednesday.\n",
      "He faces criminal solicitation, conspiracy, burglary, arson \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " \"No challenge poses more of a public threat than climate change,\" the President says .\n",
      "He credits the Clean Air Act with making Americans \"a lot\" healthier . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " President Obama says the Clean Air Act and its amendments prevented 365,000 early deaths from particulate matter alone.\n",
      "He says he wants the average American to be able to \"see the mountains in the background because they aren't covered in \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Presidential hopeful's video, featuring gay couple, gets mature rating in Russia .\n",
      "Russian TV channel feared airing it would break the country's anti-gay propaganda law .\n",
      "Clinton announced her support for same-sex marriage in 2013 . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " A Russian TV channel aired Hillary Clinton's first campaign video with a rating stamp.\n",
      "The channel said it didn't want to break the controversial law.\n",
      "Russia's anti-gay propaganda law bans \"propaganda of nontraditional sexual \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Raul Reyes: In seeking Latino vote, Marco Rubio his own worst enemy on two key issues: immigration reform, Cuba relations .\n",
      "He says on health care, climate change and other issues, he breaks from Latinos' positions. Polls show they don't favor him . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Rubio is running on an optimistic message that he embodies the promise of the American Dream.\n",
      "Rubio has been his own worst enemy on what could have been his two signature issues: immigration reform and Cuba relations.\n",
      "He holds little \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Critically acclaimed series \"Orphan Black\" returns .\n",
      "\"Turn: Washington's Spies\" starts a second season .\n",
      "\"Game of Thrones\" is back for season five . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " \"Orphan Black\" premieres on Saturday the 18th.\n",
      "The series about spies in the early days of the Revolutionary War returns with a new subtitle, \"Washington's Spies,\" and a new Monday night time slot.\n",
      " \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " The ramp agent fell asleep in the plane's cargo hold .\n",
      "He can no longer work on Alaska Airlines flights . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " A ramp agent fell asleep in the plane's cargo hold.\n",
      "He'll no longer have the option of dozing aboard one of the airline's planes.\n",
      "\"There could be a person in there so we're going to come back \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Mullah Omar, the reclusive founder of the Afghan Taliban, is still in charge, a new biography claims .\n",
      "An ex-Taliban insider says there have been rumors that the one-eyed militant is dead . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Mullah Mohammed Omar is \"still the leader\" of the Taliban's self-declared Islamic Emirate of Afghanistan.\n",
      "The biography challenges rumors of Omar's death by offering a description of his daily work schedule, which begins with prayers \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " Michelle MacLaren is no longer set to direct the first \"Wonder Woman\" theatrical movie .\n",
      "MacLaren left the project over \"creative differences\"\n",
      "Movie is currently set for 2017 . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " Michelle MacLaren is leaving the upcoming \"Wonder Woman\" movie.\n",
      "MacLaren was announced as director of the movie in November.\n",
      "The movie is still set for release on June 23, 2017. \n",
      "\n",
      "Highlight:  \n",
      " \n",
      " British tabloid releases video it says shows the robbery being carried out .\n",
      "British police say they didn't respond to a burglar alarm in jewelry district .\n",
      "Police give no value of the amount taken in the heist in London's jewelry district . \n",
      "\n",
      "Summary:  \n",
      " \n",
      " British police say they knew burglar alarm went off but didn't respond.\n",
      "A grade was applied to the call that meant no police response was required.\n",
      "The theft was so big that police haven't come up with a value for \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(res['highlights'][:10],res['pred'][:10]):\n",
    "    print('Highlight: ','\\n' ,'\\n', i, '\\n')\n",
    "    print('Summary: ', '\\n','\\n',j, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
